## Configuring Your Pipelines

The R2R library offers flexibility in customizing various aspects of the Retrieval-Augmented Generation (RAG) pipeline to meet your specific needs.

By default, RAG pipelines are configured using the R2R [`config.json`](https://github.com/SciPhi-AI/R2R/blob/main/config.json) file. You can override any fields in this configuration file with your custom settings.

## Vector Database Provider

R2R supports multiple vector database providers, including:

- **`local`**: A local vector database implementation using SQLite.
- **`qdrant`**: Integration with Qdrant, a high-performance vector similarity search engine suitable for large datasets (> 1M embedding vectors) requiring fast vector search.
- **`pgvector`**: Integration with PGVector, a vector similarity search extension for PostgreSQL, ideal for moderate-sized datasets with relational properties.
- **`sciphi`**: Managed PGVector database from SciPhi.

To specify the vector database provider, set the `provider` field under `vector_database` in the `config.json` file. Ensure you provide the necessary connection details and credentials for your chosen provider.

For more information, refer to [Vector Database Providers](/providers/vector-databases).

### Example: Qdrant Configuration

An example configuration specifying Qdrant as a vector database provider can be found in [`r2r/examples/data/example_qdrant_config.json`](https://github.com/SciPhi-AI/R2R/tree/main/r2r/examples/data/example_qdrant_config.json):

```json
{
  "vector_database": {
    "provider": "qdrant",
    "collection_name": "demo_vecs"
  }
}
```

You can serve the [R2R Demo](/getting-started/r2r-demo) using Qdrant with the following command sequence:

```bash
# Set valid Qdrant environment variables
export QDRANT_HOST=http://localhost
export QDRANT_PORT=6333
export QDRANT_API_KEY=api-...

# Serve the demo with the custom user settings
export WORKDIR=YOUR_R2R_DIRECTORY
poetry run python -m r2r.examples.demo --config_path=$WORKDIR/r2r/examples/data/example_qdrant_config.json serve
```

## Embedding Provider

R2R supports both OpenAI and local inference as embedding providers. Configure the embedding settings by updating the `embedding` section in the `config.json` file. Specify the desired embedding model, dimension, and batch size according to your requirements.

- **`openai`**: Integration with OpenAI, supporting models like `text-embedding-3-small` and `text-embedding-3-large`.
- **`sentence-transformers`**: Integration with the Sentence Transformers library, providing support for models available on HuggingFace, like `mixedbread-ai/mxbai-embed-large-v1`.

For more information, refer to [Embedding Providers](/providers/embeddings).

### Example: Local Rerank Configuration

A simple example custom user configuration can be found in [`r2r/examples/data/example_rerank_config.json`](https://github.com/SciPhi-AI/R2R/tree/main/r2r/examples/data/example_rerank_config.json):

```json
{
  "embedding": {
    "provider": "sentence-transformers",
    "search_model": "mixedbread-ai/mxbai-embed-large-v1",
    "search_dimension": 512,
    "rerank_model": "jinaai/jina-reranker-v1-turbo-en",
    "rerank_dimension": 384,
    "rerank_transformer_type": "CrossEncoder",
    "batch_size": 32,
    "text_splitter": {
      "type": "recursive_character",
      "chunk_size": 512,
      "chunk_overlap": 20
    }
  }
}
```

You can serve the [R2R Demo](/getting-started/r2r-demo) using local embeddings with rerank, as shown above, with the following command sequence:

```bash
# Be sure to install with pip install `r2r[local-llm]`

# Serve the demo with the custom user settings
export WORKDIR=YOUR_R2R_DIRECTORY
poetry run python -m r2r.examples.demo --config_path=$WORKDIR/r2r/examples/data/example_rerank_config.json serve
```

## Language Model Provider

Configure your language model provider to suit your needs:

- **`openai`**: Integration with OpenAI, supporting models like `gpt-3.5-turbo`.
- **`litellm` (default)**: Integrates with many LLM providers, such as:
  - OpenAI
  - Ollama
  - Anthropic
  - Vertex AI
  - HuggingFace
  - And more.
- **`llama-cpp`**: Integrates with the llama-cpp library for local inference.

For more information, refer to [LLM Providers](/providers/llms).

### Example: LLM Configuration

Any LLM supported by LiteLLM is immediately supported at runtime. For instance, to run with a local LLM, you may serve via Ollama and then pass the served model directly as a generation config JSON payload. See the example below:

```bash
poetry run python -m r2r.examples.demo rag --rag_generation_config='{"model":"ollama/llama2"}' --query="Who was Aristotle?"
```

## Logging Provider

The R2R library supports various logging providers to store execution logs of the RAG pipeline. Supported logging providers include:

- **`postgres`**: Logs pipeline execution information to a PostgreSQL database.
- **`local`**: Logs pipeline execution information to a local SQLite database.
- **`redis`**: Logs pipeline execution information to a Redis database.

For more information, refer to [Logging Providers](/providers/logging).

### Example: Postgres Configuration

An example configuration specifying Postgres as a logging provider can be found in [`r2r/examples/data/example_postgres_logger_config.json`](https://github.com/SciPhi-AI/R2R/tree/main/r2r/examples/data/example_postgres_logger_config.json):

```json
{
  "logging": {
    "provider": "postgres",
    "connection": {
      "user": "pg_user",
      "password": "pg_password",
      "host": "your_pg_host",
      "port": 5432,
      "dbname": "postgres"
    }
  }
}
```

You can serve the [R2R Demo](/getting-started/r2r-demo) using Postgres logging with the following command sequence:

```bash
# Set valid Postgres environment variables
export POSTGRES_USER=pg_user
export POSTGRES_PASSWORD=pg_password
export POSTGRES_HOST=http://your_pg_host
export POSTGRES_PORT=5432
export POSTGRES_DBNAME=postgres

# Serve the demo with the custom user settings
export WORKDIR=YOUR_R2R_DIRECTORY
poetry run python -m r2r.examples.demo --config_path=$WORKDIR/r2r/examples/data/example_postgres_logger_config.json serve
```

## Evaluation Provider

R2R supports LLM-powered evaluation providers, allowing you to evaluate the performance and quality of your RAG pipeline at regular intervals. Configuration is not yet fully supported for evaluation.