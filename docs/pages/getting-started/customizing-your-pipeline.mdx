## Customizing Your Pipeline

The R2R framework allows you to create custom pipelines by combining different pipes according to your specific requirements. This section demonstrates how to create a custom Retrieval-Augmented Generation (RAG) pipeline by incorporating various pipes.

### Example: Custom RAG Pipeline

To create a custom RAG pipeline, follow these steps:

1. Import the necessary modules and providers.
2. Initialize the required pipes.
3. Add the pipes to the pipeline in the desired order.

Hereâ€™s an example of creating a custom RAG pipeline with a query transformation pipe, a search pipe, and a streaming RAG pipe:

```python
"""A custom RAG pipeline that includes a custom query transformation prompt."""
from r2r import (
    GenerationConfig,
    PipeLoggingConnectionSingleton,
    R2RConfig,
    R2RPipeFactory,
    R2RProviderFactory,
    R2RQueryTransformPipe,
    RAGPipeline,
    run_pipeline,
)

# Load the default configuration
config = R2RConfig.from_json()
PipeLoggingConnectionSingleton().configure(config.logging)

# Create input providers and pipes
providers = R2RProviderFactory(config).create_providers()
pipes = R2RPipeFactory(config, providers).create_pipes()

# Add a custom prompt for transforming the user query
transform_prompt = {
    "name": "rag_fusion_prompt_custom",
    "template": "### Instruction:\n\nGiven the following query that follows to write a double newline separated list of up to {num_outputs} queries meant to help answer the original query. \nDO NOT generate any single query which is likely to require information from multiple distinct documents, \nEACH single query will be used to carry out a cosine similarity semantic search over distinct indexed documents, such as varied medical documents. \nFOR EXAMPLE if asked `how do the key themes of Great Gatsby compare with 1984`, the two queries would be \n`What are the key themes of Great Gatsby?` and `What are the key themes of 1984?`.\nHere is the original user query to be transformed into answers:\n\n### Query:\n{message}\n\n### Response:\n",
    "input_types": {"num_outputs": "int", "message": "str"},
}
providers.prompt.add_prompt(**transform_prompt)

# Initialize the new query transform pipe
query_transform_pipe = R2RQueryTransformPipe(
    llm_provider=providers.llm,
    prompt_provider=providers.prompt,
    config=R2RQueryTransformPipe.QueryTransformConfig(
        task_prompt=transform_prompt["name"]
    ),
)

# Create the RAG pipeline and add the pipes
rag_pipeline = RAGPipeline()
rag_pipeline.add_pipe(query_transform_pipe)
rag_pipeline.add_pipe(pipes.search_pipe)
rag_pipeline.add_pipe(
    pipes.rag_pipe,
    add_upstream_outputs=[
        {
            "prev_pipe_name": pipes.search_pipe.config.name,
            "prev_output_field": "search_results",
            "input_field": "raw_search_results",
        },
        {
            "prev_pipe_name": pipes.search_pipe.config.name,
            "prev_output_field": "search_queries",
            "input_field": "query",
        },
    ],
)

# Run the pipeline
result = run_pipeline(
    rag_pipeline,
    input="Who was aristotle?",
    num_query_xf_outputs=3,  # Number of transformed queries to generate
    query_transform_config=GenerationConfig(
        model="gpt-4o"
    ),  # LLM configuration for the query transformer
    rag_generation_config=GenerationConfig(
        model="gpt-4o"
    ),  # LLM configuration for the RAG model
)

print(f"Final Result:\n\n{result}")
```

# Now you can use this custom RAG pipeline for your tasks
