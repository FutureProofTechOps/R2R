## MultiRAG Example

This example demonstrates how to run a modified RAG pipeline that creates and aggregates synthetic queries and their search results to answer more complex user questions, such as "Compare the two books". For a more comprehensive example, please refer to the [Basic Example](/getting-started/basic-example).

## Step 1: Launch the MultiRAG Application Server

To launch the MultiRAG application server, run the following command:

```bash
python -m r2r.examples.servers.synthetic_query_pipeline
```

This command starts the backend server with the modified RAG pipeline, which includes the synthetic query generation and aggregation logic.

## Step 2: Understand the SyntheticRAGPipeline

The `SyntheticRAGPipeline` is a subclass of `QnARAGPipeline` that overrides the Q&A RAG pipeline logic to include synthetic query generation and aggregation. Here's an example implementation:

```python
class SyntheticRAGPipeline(QnARAGPipeline):
    def __init__(
        self,
        llm: LLMProvider,
        generation_config: GenerationConfig,
        db: VectorDBProvider,
        embedding_model: str,
        embeddings_provider: OpenAIEmbeddingProvider,
        logging_connection: Optional[LoggingDatabaseConnection] = None,
        system_prompt: Optional[str] = DEFAULT_SYSTEM_PROMPT,
        task_prompt: Optional[str] = DEFAULT_TASK_PROMPT,
    ) -> None:
        logger.debug(f"Initializing `SyntheticRAGPipeline`")
        super().__init__(
            llm,
            generation_config,
            db,
            embedding_model,
            embeddings_provider,
            logging_connection=logging_connection,
            system_prompt=system_prompt,
            task_prompt=task_prompt,
        )

    def transform_query(self, query: str) -> list[str]:
        """
        Transforms the query into a list of strings.
        """
        print(f"Transforming query: {query}")
        prompt = (
            "Use the query that follows to write a comma separated three queries "
            "that will be used to retrieve the relevant materials. DO NOT generate "
            "queries which require multiple document types. E.g. ask for `lecture notes "
            "with information about X` or `readings that cover topic Y`.\n\n## Query:\n"
            "{query}\n\n## Response:".format(query=query)
        )
        completion = self.generate_completion(prompt)
        transformed_queries = (
            completion.choices[0].message.content.strip().split("\n")
        )
        print(f"Transformed Queries: {transformed_queries}")
        return transformed_queries
```

The `SyntheticRAGPipeline` overrides the `transform_query` method to generate synthetic queries based on the user's original query. It uses a prompt to guide the LLM in generating relevant queries that can be used to retrieve the necessary materials.

## Step 3: Run the MultiRAG Client

To demonstrate the MultiRAG functionality, you can run the example client script using the following command:

```bash
python -m r2r.examples.clients.run_synthetic_query_client
```

This command runs the client script that interacts with the MultiRAG application server. It sends a user query to the server, which then generates synthetic queries, retrieves relevant search results, and aggregates them to provide a comprehensive answer to the original query.

The client script demonstrates how to communicate with the MultiRAG application server and retrieve the generated answers.

By following these steps, you can run the MultiRAG example, which showcases a modified RAG pipeline that generates and aggregates synthetic queries to answer more complex user questions. This example highlights the flexibility and extensibility of the R2R framework in implementing advanced retrieval and generation techniques.