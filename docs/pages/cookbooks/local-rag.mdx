import { Tabs } from 'nextra/components'
import { Callout } from 'nextra/components'

## Building a Local RAG System with R2R

### Installation

**This document extends the [R2R Quickstart](/getting-started/quickstart)** by demonstrating how R2R can be ran with a local large language model (LLM).

<Tabs items={['Installing with Pip 🐍', 'Installing with Docker 🐳']}>
  <Tabs.Tab>
    Running R2R with local RAG requires an additional `local-embedding` dependency to support local embedding creation:
    ```bash copy
      pip install 'r2r[local-embedding]'
    ```
  </Tabs.Tab>

  <Tabs.Tab>

    <Callout>
    When running with Docker, `--client_server_mode` must be appended to all the commands shown below, and the flag `--config_name=local_ollama` will no longer have any impact. For more information on the client-server architecture, read the [relevant cookbook here](/cookbooks/client-server).
    </Callout>


    Run your Docker container using the following command:

    ```bash filename="bash" copy
    docker run -d \
      --name r2r \
      --add-host=host.docker.internal:host-gateway \
      -p 8000:8000 \
      -e OLLAMA_API_BASE=http://host.docker.internal:11434 \
      -e CONFIG_OPTION=local_ollama \
      emrgntcmplxty/r2r:latest
    ```

    This command starts the R2R container with the following options:

    - `--name r2r`: Assigns the name "r2r" to the container.
    - `--add-host=host.docker.internal:host-gateway`: Adds a host entry for the Docker host.
    - `-p 8000:8000`: Maps port 8000 of the container to port 8000 of the host.
    - `-e OLLAMA_API_BASE=http://host.docker.internal:11434`: Specifies the Ollama API base URL.
    - `-e CONFIG_OPTION=local_ollama`: Selects the "local_ollama" configuration option.
    - `emrgntcmplxty/r2r:latest`: Specifies the Docker image to use.

  </Tabs.Tab>
</Tabs>

By default R2R supports `Ollama`, a popular tool for Local LLM inference, and it will be used in the cookbook that follows.

Ollama must be installed independently. You can install Ollama by following the instructions on their [official website](https://ollama.com/) or by referring to their [GitHub README](https://github.com/ollama/ollama).

### Configuration

Let's move on to setting up the R2R pipeline. R2R relies on a `config.json` file for defining various settings, such as embedding models and chunk sizes. By default, the `config.json` found in the R2R GitHub repository's root directory is set up for cloud-based services. We must run with a separate configuration in order to support local settings, this is the `local_ollama` config we selected previously.


<details>
<summary>Local Configuration</summary>

For setting up an on-premises RAG system, we need to adjust the configuration to use local resources. For this example, this only involves changing the embedding provider as our default LLM provider integrates seamlessly with Ollama.

To streamline this process, we've provided pre-configured local settings in the [`examples/configs`](https://github.com/SciPhi-AI/R2R/blob/main/r2r/examples/configs) directory, named `local_ollama`. We've included a printout of the config below for your convenience:

```json
{
  "embedding": {
    "provider": "sentence-transformers",
    "base_model": "all-MiniLM-L6-v2",
    "base_dimension": 384,
    "batch_size": 32
  },
  "eval": {
    "provider": "local",
    "frequency": 0.0,
    "llm":{
      "provider": "litellm"
    }
  },
  "ingestion":{
    "excluded_parsers": {
      "gif": "default",
      "jpeg": "default",
      "jpg": "default",
      "png": "default",
      "svg": "default",
      "mp3": "default",
      "mp4": "default"
    }
  }
}
```

You may also modify the configuration defaults for ingestion, logging, and your vector database provider in a similar manner. More information on these settings are included in this coookbook and throughout the documentation.

This chosen config modification above instructs R2R to use the `sentence-transformers` library for embeddings with the `all-MiniLM-L6-v2` model, turns off evals, and sets the LLM provider to `ollama`. During ingestion, the default is to split documents into chunks of 512 characters with 20 characters of overlap between chunks.

A local vector database will be used to store the embeddings. The current default is a minimal sqlite implementation.
</details>


## Ingesting and Embedding Documents

With our environment set up and our server running in a separate process, we're ready to ingest a document! As an example, R2R includes a several files such as Aristotle's wikipedia page in `.txt` an assortment of Paul Graham's essays in `.html` and a few 10ks in `.pdf`. This file ships with the package and is included by default.

Run this command to ingest the document, skipping media as multi-modal RAG is not yet supported when running locally:

```bash filename="bash" copy
python -m r2r.examples.quickstart ingest_as_files --no-media=true --config_name=local_ollama
```

The output should look something like this:

```
...
{'results':
  [
    "File 'aristotle.txt' processed successfully.",
    "File 'pg_essay_1.html' processed successfully.",
    "File 'pg_essay_2.html' processed successfully.",
    "File 'pg_essay_3.html' processed successfully.",
    "File 'pg_essay_4.html' processed successfully.",
    "File 'pg_essay_5.html' processed successfully.",
    "File 'lyft_2021.pdf' processed successfully.",
    "File 'uber_2021.pdf' processed successfully."
  ]
}
```

Here's what's happening under the hood:
1. R2R loads and processes the document.
2. It splits the text into chunks of 512 characters each, with 20 characters overlapping between chunks.
3. Each chunk is embedded using the `all-MiniLM-L6-v2` model from `sentence-transformers`.
4. The chunks and embeddings are stored in the specified vector database, which defaults to a local SQLite database.

With just one command, we've gone from a raw document to an embedded knowledge base we can query. In addition to the raw chunks, metadata such as user ID or document ID can be attached to enable easy filtering later. For more information on similar functionalities provided by the quickstart script, refer to the [R2R Quickstart](/getting-started/quickstart).

### Running Queries on the Local LLM

Time for the fun part—asking questions!

To query our knowledge base using Ollama, first ensure that you have started the Ollama server with this command in the terminal:

```bash filename="bash" copy
ollama serve llama2
```

<Callout type="warning" emoji="️⚠️">
   Be sure to run client commands in a new terminal after starting the ollama server!
</Callout>


Then, to ask a question, run:

```bash filename="bash" copy
python -m r2r.examples.quickstart rag \
  --query="What contributions did Aristotle make to biology?" \
  --config_name=local_ollama \
  --rag_generation_config='{"model": "ollama/llama2"}'
```

This command tells R2R to use the specified model to generate a completion for the given query. R2R will:
1. Embed the query using `all-MiniLM-L6-v2`.
2. Find the chunks most similar to the query embedding.
3. Pass the query and relevant chunks to the LLM to generate a response.

After a brief wait, you should see the LLM's response, perhaps something like:

> ### Aristotle's Contributions to Biology
>
> Aristotle's work has had a significant impact on the field of biology, earning him the title "father of biology." His contributions to the field include:
>
> 1. **Systematic observation and classification of living things**: Aristotle was one of the first people to study biology systematically, laying the foundation for modern taxonomy.
> 2. **Development of the concept of evolution**: Aristotle recognized that living things change over time and proposed a mechanism for these changes (the "great chain of being"). While his ideas about evolution were not entirely correct, they were an important precursor to the modern theory of evolution.
> 3. **Influence on medieval scholarship**: Aristotle's work profoundly impacted medieval scholarship, shaping the field of biology for centuries after his death.
> 4. **Contributions to logic and metaphysics**: Aristotle's work in logic and metaphysics also influenced the development of biology, as these areas of study inform our understanding of the natural world.

How cool is that? With R2R, we've built a simple Q&A system backed by a local LLM that we can converse with. The best part is, all the data and compute stayed on our own machine!

### Configuring Your RAG Pipeline

R2R provides flexibility in customizing various aspects of the RAG pipeline to suit your needs. This can be done through the `config.json` file. Some key configuration options include:

#### Vector Database Provider
R2R supports multiple vector database providers:
- `local`: A SQLite-based local vector database
- `pgvector`: Integration with pgvector extension for Postgres

Set the `provider` field under `vector_database` in `config.json` to specify your provider.

#### Embedding Provider
R2R supports OpenAI and local inference embedding providers:
- `openai`: OpenAI models like `text-embedding-3-small`
- `sentence-transformers`: HuggingFace models like `all-MiniLM-L6-v2`

Configure the `embedding` section to set your desired embedding model, dimension, and batch size.

#### Language Model Provider
- `openai`: Models like `gpt-3.5-turbo`
- `litellm` (default): Integrates with many providers (OpenAI, Anthropic, Vertex AI, etc.)
- `ollama`: A specifically supported provider, with the connection managed by `litellm`

#### Logging Provider
R2R supports logging to Postgres, SQLite (local), and Redis. The logs capture pipeline execution information.

Check out the [full R2R configuration docs](/deep-dive/config) for more details on all available options.

### Next Steps

Together we've walked through the steps to get a local LLM application up and running with R2R:
1. Installing dependencies
2. Configuring the R2R pipeline
3. Ingesting and embedding documents
4. Running queries on a local LLM

But we've only scratched the surface of what you can build with R2R! I encourage you to keep experimenting - try ingesting your own documents, customizing your pipeline, tweaking model parameters, and exploring R2R's evaluation capabilities.

If you have any questions or want to learn more, [R2R's GitHub repo](https://github.com/SciPhi-ai/R2R) and the [R2R Discord](https://discord.gg/p6KqD2kjtB) are great places to get started.
