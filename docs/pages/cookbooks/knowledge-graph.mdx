import { Callout, FileTree } from 'nextra/components';


## Knowledge Graphs with R2R

<Callout type="info" emoji="⚠️">
   Knowledge graph features in R2R are still under development.
</Callout>

This cookbook explains how to configure R2R to automatically construct a knowledge graph from ingested files. The constructed graph will then be used in downstream RAG pipelines as an additional knowledge base in your R2R application.

### Setup

To enable R2R knowledge graph construction, you must specify a provider for `kg` in your R2R `config.json`. The following code can be found in `r2r/examples/configs/neo4j_kg.json` and contains the necessary settings for RAG graph construction:

```json filename="neo4j_kg.json" copy
{
  "kg": {
    "provider": "neo4j",
    "batch_size": 1,
    "text_splitter": {
      "type": "recursive_character",
      "chunk_size": 1024,
      "chunk_overlap": 0
    }
  }
}
```

After selecting a knowledge graph provider, the default R2R `IngestionPipeline` will simultaneously construct a knowledge graph and vector search engine during ingestion. Setting `batch_size=1` restricts the system such that one LLM call is processed at a time, and setting `chunk_size=1024` defines the size of chunks used during extraction. The key logic behind the knowledge graph pipeline is contained in the `kg_pipe.py` and `kg_storage.py` which are constructed by the `R2RPipelineFactory` class.

We must set valid environment variables to enable communication with the Neo4j database. For this cookbook, we assume the reader has already downloaded the desktop application [found here](https://neo4j.com/download) and is running with version `>=5.13.0` and `APOC` installed.

```bash
export NEO4J_USER=neo4j
export NEO4J_PASSWORD=password
export NEO4J_URL=bolt://localhost:7687
export NEO4J_DATABASE=neo4j
```

As in other cookbooks, this configuration file can be loaded directly into an R2R server as shown below. However, for this demonstration this is not required.

```bash filename="bash" copy
# cd $WORKDIR
# export NEO4J_...
# export OPENAI_API_KEY=...
python -m r2r.examples.servers.configurable_pipeline --host 0.0.0.0 --port 8000 --config neo4j_kg.json
```

<details>
<summary>Relevant Factory Source Code</summary>

```python filename="r2r_factory.py" copy
class R2RPipelineFactory:
  ...
  def create_ingestion_pipeline(self, *args, **kwargs) -> IngestionPipeline:
      """Factory method to create an ingestion pipeline."""
      ingestion_pipeline = IngestionPipeline()

      ingestion_pipeline.add_pipe(
          pipe=self.pipes.parsing_pipe, parsing_pipe=True
      )
      # Add embedding pipes if provider is set
      if self.config.embedding.provider:
          ingestion_pipeline.add_pipe(
              self.pipes.embedding_pipe, embedding_pipe=True
          )
          ingestion_pipeline.add_pipe(
              self.pipes.vector_storage_pipe, embedding_pipe=True
          )
      # Add KG pipes if provider is set
      if self.config.kg.provider:
          ingestion_pipeline.add_pipe(self.pipes.kg_pipe, kg_pipe=True)
          ingestion_pipeline.add_pipe(
              self.pipes.kg_storage_pipe, kg_pipe=True
          )

      return ingestion_pipeline
```
</details>

### Implementation

We are now ready to construct an example knowledge graph. As configured, R2R will extract named entities and their relationships as documents and files are ingested. The snippet below will build an instance of the R2R with the correct settings:

```python filename="r2r/examples/scripts/build_yc_kg.py"
from r2r import R2RAppBuilder, R2RConfig

# Load the R2R configuration and build the app
this_file_path = os.path.abspath(os.path.dirname(__file__))
config_path = os.path.join(this_file_path, "..", "configs", "neo4j_kg.json")
config = R2RConfig.from_json(config_path)
r2r = R2RAppBuilder(config).build()
```

We will be ingesting live startup data from the YCombinator company directory for this cookbook. For best results we will override the default prompt for knowledge graph construction, `ner_kg_extraction`, which can be found in `prompts/local/defaults.jsonl`. This default prompt constructs the graph in a way that is agnostic to the input data. Instead, we will overwrite this prompt with one that contains specific entities and predicates relevant to startups.

```python filename="r2r/examples/scripts/build_yc_kg.py"
from r2r import EntityType, Relation, format_entity_types, format_relations

def update_kg_prompt(
    prompt_provider,
    entity_types,
    relations,
    kg_extraction_prompt_name="ner_kg_extraction",
):
    # Fetch the kg extraction prompt with blank entity types and relations
    ner_kg_extraction_with_spec = prompt_provider.get_prompt(
        "ner_kg_extraction_with_spec"
    )

    # Format the prompt to include the desired entity types and relations
    ner_kg_extraction = ner_kg_extraction_with_spec.replace(
        "{entity_types}", format_entity_types(entity_types)
    ).replace("{relations}", format_relations(relations))

    # Update the "ner_kg_extraction" prompt used in downstream KG construction
    prompt_provider.update_prompt(
        kg_extraction_prompt_name,
        json.dumps(ner_kg_extraction, ensure_ascii=False),
    )

# Get the prompt provider
prompt_provider = r2r.providers.prompt

# Specify the entity types for the KG extraction prompt
entity_types = [
    EntityType(
        "ORGANIZATION",
        subcategories=["COMPANY", "SCHOOL", "NON-PROFIT", "OTHER"],
    ),
    EntityType(
        "LOCATION", subcategories=["CITY", "STATE", "COUNTRY", "OTHER"]
    ),
    EntityType("PERSON"),
    EntityType("POSITION"),
    EntityType(
        "DATE",
        subcategories=[
            "YEAR",
            "MONTH",
            "DAY",
            "BATCH (E.G. W24, S20)",
            "OTHER",
        ],
    ),
    EntityType("QUANTITY"),
    EntityType(
        "EVENT",
        subcategories=[
            "INCORPORATION",
            "FUNDING_ROUND",
            "ACQUISITION",
            "LAUNCH",
            "OTHER",
        ],
    ),
    EntityType("INDUSTRY"),
    EntityType(
        "MEDIA",
        subcategories=["EMAIL", "WEBSITE", "TWITTER", "LINKEDIN", "OTHER"],
    ),
    EntityType("PRODUCT"),
]

# Specify the relations for the KG construction
relations = [
    # Founder Relations   
    Relation("EDUCATED_AT"),
    Relation("WORKED_AT"),
    Relation("FOUNDED"),
    # Company relations
    Relation("RAISED"),
    Relation("REVENUE"),
    Relation("TEAM_SIZE"),
    Relation("LOCATION"),
    Relation("ACQUIRED_BY"),
    Relation("ANNOUNCED"),
    Relation("INDUSTRY"),
    # Product relations
    Relation("PRODUCT"),
    Relation("FEATURES"),
    Relation("USES"),
    Relation("USED_BY"),
    Relation("TECHNOLOGY"),
    # Additional relations
    Relation("HAS"),
    Relation("AS_OF"),
    Relation("PARTICIPATED"),
    Relation("ASSOCIATED"),
    Relation("GROUP_PARTNER"),
    Relation("ALIAS"),
]

# Update the KG extraction prompt with the specified entity types and relations
update_kg_prompt(prompt_provider, entity_types, relations)
```

Finally, we are ready to ingest data. The referenced cookbook script includes code to scrape and ingest up to 1,000 companies from the YC directory. The relevant code for ingestion is shown below:

```python filename="r2r/examples/scripts/build_yc_kg.py"
from r2r import Document

i = 0
# Ingest and clean the data for each company
for company, url in url_map.items():
    company_data = fetch_and_clean_yc_co_data(url) # defined above
    if i >= max_entries:
        break
    # Wrap in a try block in case of network failure
    try:
        # Ingest as a text document
        r2r.ingest_documents(
            [
                Document(
                    id=generate_id_from_label(company),
                    type="txt",
                    data=company_data,
                    metadata={},
                )
            ]
        )
    except:
        continue
    i += 1

print_all_relationships(kg_provider)
```

Let's start by scraping one example to get an initial sense of the data quality produced by this technique.

```bash
python r2r/examples/scripts/build_yc_kg.py --max_entries=1
```

Running the command above will produce output like the following:

```bash
PERSON:Joe Gebbia -[FOUNDED]-> ORGANIZATION:COMPANY:Airbnb
ORGANIZATION:COMPANY:Airbnb -[LOCATION]-> LOCATION:CITY:San Francisco
ORGANIZATION:COMPANY:Airbnb -[HAS]-> QUANTITY:7M listings
ORGANIZATION:COMPANY:Airbnb -[LOCATION]-> LOCATION:COUNTRY:191+ countries
PERSON:Joe Gebbia -[WORKED_AT]-> ORGANIZATION:COMPANY:Airbnb
PERSON:Joe Gebbia -[FOUNDED]-> ORGANIZATION:COMPANY:Samara
ORGANIZATION:COMPANY:Samara -[PRODUCT]-> PRODUCT:fully customized, factory-made homes
ORGANIZATION:COMPANY:Samara -[INDUSTRY]-> INDUSTRY:economic empowerment
ORGANIZATION:COMPANY:Samara -[INDUSTRY]-> INDUSTRY:housing resources
ORGANIZATION:COMPANY:Samara -[INDUSTRY]-> INDUSTRY:design
ORGANIZATION:COMPANY:Airbnb -[FOUNDED]-> DATE:MONTH:August
ORGANIZATION:COMPANY:Airbnb -[FOUNDED]-> DATE:YEAR:2008
ORGANIZATION:COMPANY:Airbnb -[LOCATION]-> LOCATION:STATE:California
ORGANIZATION:COMPANY:Airbnb -[HAS]-> QUANTITY:more than 33,000 cities
...
```

We are now ready to run a much larger ingestion.

```bash
poetry run python r2r/examples/scripts/build_yc_kg.py --delete --max_entries=100
```

### Results


After successful completion we will see a large printout showing the various connections that have been formed across our knowledge graph. Indeed, there are more connections than we can readily understand. We use the Neo4j browser which ships with the desktop application to gain us some intuition around the constructed graph.

<br />

<figure>
  <img src="/graph.png" alt="Industry Relationships"/>
  <figcaption>Figure 1: Graph showing industry relationships.</figcaption>
</figure>

<br />

We can see a rich and nuanced structure has naturally emerged, with SaaS being the centralmost node.
<figure>
  <img src="/graph2.png" alt="Founder Relationships"/>
  <figcaption>Figure 2: Graph showing founder relationships, focused on Owen Colegrove.</figcaption>
</figure>


Using the snippets that follow we can begin to explore the graph through targeted queries.

```python

def execute_query(provider, query, params={}):
    print(f"Executing query: {query}")
    with provider.client.session(database=provider._database) as session:
        result = session.run(query, params)
        return [record.data() for record in result]

```


Executing a few queries, we see outputs like the following:

```bash
## Locating all founders

Executing query: 
MATCH (p)-[:FOUNDED]->(c)
WHERE p.id CONTAINS 'PERSON:'
RETURN p.id AS Founder, c.id AS Company
ORDER BY c.id
LIMIT 10;

results =  [{'Founder': 'PERSON:Sidharth', 'Company': 'EVENT:second startup'}, {'Founder': 'PERSON:Sourav', 'Company': 'EVENT:second startup'}, {'Founder': 'PERSON:Stefan', 'Company': 'MEDIA:WEBSITE:https://github.com/dagworks-inc/hamilton'}, {'Founder': 'PERSON:Dawson Chen', 'Company': 'NON-PROFIT:Nonprofit to find employment for the homeless'}, {'Founder': 'PERSON:Joe Gebbia', 'Company': 'ORGANIZATION:COMPANY:Airbnb'}, {'Founder': 'PERSON:Brian Chesky', 'Company': 'ORGANIZATION:COMPANY:Airbnb'}, {'Founder': 'PERSON:Joanne Wang', 'Company': 'ORGANIZATION:COMPANY:Airfront'}, {'Founder': 'PERSON:Tommy Guo', 'Company': 'ORGANIZATION:COMPANY:Airfront'}, {'Founder': 'PERSON:Usman Gul', 'Company': 'ORGANIZATION:COMPANY:Airlift'}, {'Founder': 'PERSON:Adam Tilton', 'Company': 'ORGANIZATION:COMPANY:Aktive'}]
```


```bash
## Locating 2-time founders

Executing query: 
MATCH (p)-[:FOUNDED]->(c)
WHERE p.id CONTAINS 'PERSON:' AND c.id CONTAINS 'COMPANY:'
WITH p.id AS Person, COUNT(c) AS CompaniesCount
WHERE CompaniesCount > 1
RETURN Person, CompaniesCount
ORDER BY CompaniesCount DESC;

results =  [{'Person': 'PERSON:Joe Gebbia', 'CompaniesCount': 2}, {'Person': 'PERSON:Ilana Nasser', 'CompaniesCount': 2}, {'Person': 'PERSON:Sourav Choraria', 'CompaniesCount': 2}, {'Person': 'PERSON:Kris Pahuja', 'CompaniesCount': 2}, {'Person': 'PERSON:Adam Tilton', 'CompaniesCount': 2}, {'Person': 'PERSON:Daniel Hensley', 'CompaniesCount': 2}, {'Person': 'PERSON:Sam', 'CompaniesCount': 2}, {'Person': 'PERSON:Jason Kerkvliet', 'CompaniesCount': 2}, {'Person': 'PERSON:Tom Blomfield', 'CompaniesCount': 2}, {'Person': 'PERSON:Usman Gul', 'CompaniesCount': 2}, {'Person': 'PERSON:Lucious', 'CompaniesCount': 2}]
```



```bash
### Locating companies with AI products

Executing query: 
MATCH (e)-[r:PRODUCT]->(t)
WHERE t.id CONTAINS 'AI'
RETURN e.id AS Entity, t.id AS Product
ORDER BY e.id;

results =  [{'Entity': 'ORGANIZATION:COMPANY:AgentsForce', 'Product': 'PRODUCT:AI support agents'}, {'Entity': 'ORGANIZATION:COMPANY:Airfront', 'Product': 'PRODUCT:AI-first email platform'}, {'Entity': 'ORGANIZATION:COMPANY:Airfront', 'Product': 'PRODUCT:AI-first email platform with built-in automations'}, {'Entity': 'ORGANIZATION:COMPANY:Axflow', 'Product': 'PRODUCT:Tools for product teams building AI applications'}, {'Entity': 'ORGANIZATION:COMPANY:Axflow', 'Product': 'PRODUCT:AI app'}, {'Entity': 'ORGANIZATION:COMPANY:Clarum', 'Product': 'PRODUCT:AI-powered due diligence solutions'}, {'Entity': 'ORGANIZATION:COMPANY:Clarum', 'Product': 'PRODUCT:AI-powered due diligence'}, {'Entity': 'ORGANIZATION:COMPANY:Clerkie', 'Product': 'PRODUCT:AI tool'}, {'Entity': 'ORGANIZATION:COMPANY:Dawn', 'Product': 'PRODUCT:Analytics for AI products'}, {'Entity': 'ORGANIZATION:COMPANY:Decipher', 'Product': 'PRODUCT:Decipher AI'}, {'Entity': 'ORGANIZATION:COMPANY:Decipher', 'Product': 'PRODUCT:AI-powered user impact summaries'}, {'Entity': 'ORGANIZATION:COMPANY:EzDubs', 'Product': 'PRODUCT:AI dubbing'}, {'Entity': 'ORGANIZATION:COMPANY:EzDubs', 'Product': 'PRODUCT:Real-time AI dubbing'}, {'Entity': 'ORGANIZATION:COMPANY:EzDubs', 'Product': 'PRODUCT:AI video dubbing tool'}, {'Entity': 'ORGANIZATION:COMPANY:Flint', 'Product': 'PRODUCT:AI platform for K-12'}, {'Entity': 'ORGANIZATION:COMPANY:Focal', 'Product': 'PRODUCT:AI video creation tool'}, {'Entity': 'ORGANIZATION:COMPANY:Focal', 'Product': 'PRODUCT:AI movie studio'}, {'Entity': 'ORGANIZATION:COMPANY:Fynt', 'Product': 'PRODUCT:Fynt - The AI Financial Controller'}, {'Entity': 'ORGANIZATION:COMPANY:Fynt AI', 'Product': 'PRODUCT:Fynt - The AI Financial Controller'}, {'Entity': 'ORGANIZATION:COMPANY:Lightski', 'Product': 'PRODUCT:AI Data Scientist'}, {'Entity': 'ORGANIZATION:COMPANY:Lightski', 'Product': 'PRODUCT:Lightski AI Data Scientist'}, {'Entity': 'ORGANIZATION:COMPANY:Magic Hour', 'Product': 'PRODUCT:AI video generation'}, {'Entity': 'ORGANIZATION:COMPANY:MagnaPlay', 'Product': 'PRODUCT:AI-powered game translation'}, {'Entity': 'ORGANIZATION:COMPANY:Magnaplay', 'Product': 'PRODUCT:AI-powered game translation'}, {'Entity': 'ORGANIZATION:COMPANY:Narrative', 'Product': 'PRODUCT:AI-powered invoice audit, analytics and payments tool'}, {'Entity': 'ORGANIZATION:COMPANY:Octo', 'Product': 'PRODUCT:AI agents'}, {'Entity': 'ORGANIZATION:COMPANY:Osium AI', 'Product': 'PRODUCT:AI-powered software'}, {'Entity': 'ORGANIZATION:COMPANY:ProSights', 'Product': 'PRODUCT:AI-native operating system'}, {'Entity': 'ORGANIZATION:COMPANY:Raz', 'Product': 'PRODUCT:Next-generation AI forms'}, {'Entity': 'ORGANIZATION:COMPANY:Relari', 'Product': 'PRODUCT:Testing and simulation stack for GenAI systems'}, {'Entity': 'ORGANIZATION:COMPANY:Relari', 'Product': 'PRODUCT:Testing and Simulation Stack for GenAI Systems'}, {'Entity': 'ORGANIZATION:COMPANY:Relari.ai', 'Product': 'PRODUCT:GenAI applications'}, {'Entity': 'ORGANIZATION:COMPANY:Slicker', 'Product': 'PRODUCT:AI-powered payments platform'}, {'Entity': 'ORGANIZATION:COMPANY:Slicker', 'Product': 'PRODUCT:AI-powered modular payments infrastructure'}, {'Entity': 'ORGANIZATION:COMPANY:Spark', 'Product': 'PRODUCT:AI-powered planning and workflow solution'}, {'Entity': 'ORGANIZATION:COMPANY:TableFlow', 'Product': 'PRODUCT:AI-Powered Data Transformation'}, {'Entity': 'ORGANIZATION:COMPANY:Wild Moose', 'Product': 'PRODUCT:Gen AI'}, {'Entity': 'ORGANIZATION:COMPANY:ego', 'Product': 'PRODUCT:AI-native 3D simulation engine'}, {'Entity': 'ORGANIZATION:COMPANY:tl;dr ego', 'Product': 'PRODUCT:AI-native simulation engine'}, {'Entity': 'PRODUCT:Octo', 'Product': 'PRODUCT:AI agents'}, {'Entity': 'PRODUCT:Upstream', 'Product': 'MEDIA:EMAIL'}]
```

```bash
## Locating AI founders with optional education info

Executing query: 
MATCH (p)-[:FOUNDED]->(c)-[:INDUSTRY]->(i)
WHERE p.id CONTAINS 'PERSON:' AND i.id CONTAINS 'AI'
WITH p, c, i
OPTIONAL MATCH (p)-[:EDUCATED_AT]->(s)
RETURN p.id AS Founder, c.id AS Company, i.id AS Industry, collect(s.id) AS Schools
ORDER BY p.id
LIMIT 10;

results =  [{'Founder': 'PERSON:Jean-Michael Diei', 'Company': 'ORGANIZATION:COMPANY:PropRise', 'Industry': 'INDUSTRY:AI-driven automation', 'Schools': [], 'Honors': []}, {'Founder': 'PERSON:Pedro', 'Company': 'ORGANIZATION:COMPANY:Flint', 'Industry': 'INDUSTRY:AI tutoring for schools', 'Schools': ['ORGANIZATION:SCHOOL:Harvard'], 'Honors': []}]
```

