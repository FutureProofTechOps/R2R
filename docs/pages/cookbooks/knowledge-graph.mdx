import { Callout, FileTree } from 'nextra/components';


## Knowledge Graphs with R2R

<Callout type="info" emoji="⚠️">
   Knowledge graph features in R2R are still under development.
</Callout>

This cookbook explains how to configure R2R to automatically construct a knowledge graph from ingested files. The constructed graph will then be used in downstream RAG pipelines as an additional knowledge base in your R2R application.

### Setup

To enable R2R knowledge graph construction, you must specify a provider for `kg` in your R2R `config.json`. The following code can be found in `r2r/examples/configs/neo4j_kg.json` and contains the necessary settings for RAG graph construction:

```json filename="neo4j_kg.json" copy
{
  "kg": {
    "provider": "neo4j",
    "batch_size": 1,
    "text_splitter": {
      "type": "recursive_character",
      "chunk_size": 1024,
      "chunk_overlap": 0
    }
  }
}
```

After selecting a knowledge graph provider, the default R2R `IngestionPipeline` will simultaneously construct a knowledge graph and vector search engine during ingestion. Setting `batch_size=1` restricts the system such that one LLM call is processed at a time, and setting `chunk_size=1024` defines the size of chunks used during extraction. The key logic behind the knowledge graph pipeline is contained in the `kg_pipe.py` and `kg_storage.py` which are constructed by the `R2RPipelineFactory` class.

We must set valid environment variables to enable communication with the Neo4j database. For this cookbook, we assume the reader has already downloaded the desktop application [found here](https://neo4j.com/download) and is running with version `>=5.13.0` and `APOC` installed.

```bash
export NEO4J_USER=neo4j
export NEO4J_PASSWORD=password
export NEO4J_URL=bolt://localhost:7687
export NEO4J_DATABASE=neo4j
```

As in other cookbooks, this configuration file can be loaded directly into an R2R server as shown below. However, for this demonstration this is not required.

```bash filename="bash" copy
# cd $WORKDIR
# export NEO4J_...
# export OPENAI_API_KEY=...
python -m r2r.examples.servers.configurable_pipeline --host 0.0.0.0 --port 8000 --config neo4j_kg.json
```

<details>
<summary>Relevant Factory Source Code</summary>

```python filename="r2r_factory.py" copy
class R2RPipelineFactory:
  ...
  def create_ingestion_pipeline(self, *args, **kwargs) -> IngestionPipeline:
      """Factory method to create an ingestion pipeline."""
      ingestion_pipeline = IngestionPipeline()

      ingestion_pipeline.add_pipe(
          pipe=self.pipes.parsing_pipe, parsing_pipe=True
      )
      # Add embedding pipes if provider is set
      if self.config.embedding.provider:
          ingestion_pipeline.add_pipe(
              self.pipes.embedding_pipe, embedding_pipe=True
          )
          ingestion_pipeline.add_pipe(
              self.pipes.vector_storage_pipe, embedding_pipe=True
          )
      # Add KG pipes if provider is set
      if self.config.kg.provider:
          ingestion_pipeline.add_pipe(self.pipes.kg_pipe, kg_pipe=True)
          ingestion_pipeline.add_pipe(
              self.pipes.kg_storage_pipe, kg_pipe=True
          )

      return ingestion_pipeline
```
</details>

### Implementation

We are now ready to construct an example knowledge graph. As configured, R2R will extract named entities and their relationships as documents and files are ingested. The snippet below will build an instance of the R2R with the correct settings:

```python filename="r2r/examples/scripts/build_yc_kg.py"
from r2r import R2RAppBuilder, R2RConfig

# Load the R2R configuration and build the app
this_file_path = os.path.abspath(os.path.dirname(__file__))
config_path = os.path.join(this_file_path, "..", "configs", "neo4j_kg.json")
config = R2RConfig.from_json(config_path)
r2r = R2RAppBuilder(config).build()
```

We will be ingesting live startup data from the YCombinator company directory for this cookbook. For best results we will override the default prompt for knowledge graph construction, `ner_kg_extraction`, which can be found in `prompts/local/defaults.jsonl`. This default prompt constructs the graph in a way that is agnostic to the input data. Instead, we will overwrite this prompt with one that contains specific entities and predicates relevant to startups.

```python filename="r2r/examples/scripts/build_yc_kg.py"
from r2r import EntityType, Relation, format_entity_types, format_relations

def update_kg_prompt(
    prompt_provider,
    entity_types,
    relations,
    kg_extraction_prompt_name="ner_kg_extraction",
):
    # Fetch the kg extraction prompt with blank entity types and relations
    ner_kg_extraction_with_spec = prompt_provider.get_prompt(
        "ner_kg_extraction_with_spec"
    )

    # Format the prompt to include the desired entity types and relations
    ner_kg_extraction = ner_kg_extraction_with_spec.replace(
        "{entity_types}", format_entity_types(entity_types)
    ).replace("{relations}", format_relations(relations))

    # Update the "ner_kg_extraction" prompt used in downstream KG construction
    prompt_provider.update_prompt(
        kg_extraction_prompt_name,
        json.dumps(ner_kg_extraction, ensure_ascii=False),
    )

# Get the prompt provider
prompt_provider = r2r.providers.prompt

# Specify the entity types for the KG extraction prompt
entity_types = [
    EntityType(
        "ORGANIZATION",
        subcategories=["COMPANY", "SCHOOL", "NON-PROFIT", "OTHER"],
    ),
    EntityType(
        "LOCATION", subcategories=["CITY", "STATE", "COUNTRY", "OTHER"]
    ),
    EntityType("PERSON"),
    EntityType("POSITION"),
    EntityType(
        "DATE",
        subcategories=[
            "YEAR",
            "MONTH",
            "DAY",
            "BATCH (E.G. W24, S20)",
            "OTHER",
        ],
    ),
    EntityType("QUANTITY"),
    EntityType(
        "EVENT",
        subcategories=[
            "INCORPORATION",
            "FUNDING_ROUND",
            "ACQUISITION",
            "LAUNCH",
            "OTHER",
        ],
    ),
    EntityType("INDUSTRY"),
    EntityType(
        "MEDIA",
        subcategories=["EMAIL", "WEBSITE", "TWITTER", "LINKEDIN", "OTHER"],
    ),
    EntityType("PRODUCT"),
]

# Specify the relations for the KG construction
relations = [
    # Founder Relations   
    Relation("EDUCATED_AT"),
    Relation("WORKED_AT"),
    Relation("FOUNDED"),
    # Company relations
    Relation("RAISED"),
    Relation("REVENUE"),
    Relation("TEAM_SIZE"),
    Relation("LOCATION"),
    Relation("ACQUIRED_BY"),
    Relation("ANNOUNCED"),
    Relation("INDUSTRY"),
    # Product relations
    Relation("PRODUCT"),
    Relation("FEATURES"),
    Relation("USES"),
    Relation("USED_BY"),
    Relation("TECHNOLOGY"),
    # Additional relations
    Relation("HAS"),
    Relation("AS_OF"),
    Relation("PARTICIPATED"),
    Relation("ASSOCIATED"),
    Relation("GROUP_PARTNER"),
    Relation("ALIAS"),
]

# Update the KG extraction prompt with the specified entity types and relations
update_kg_prompt(prompt_provider, entity_types, relations)
```

Finally, we are ready to ingest data. The referenced cookbook script includes code to scrape and ingest up to 1,000 companies from the YC directory. The relevant code for ingestion is shown below:

```python filename="r2r/examples/scripts/build_yc_kg.py"
from r2r import Document

i = 0
# Ingest and clean the data for each company
for company, url in url_map.items():
    company_data = fetch_and_clean_yc_co_data(url) # defined above
    if i >= max_entries:
        break
    # Wrap in a try block in case of network failure
    try:
        # Ingest as a text document
        r2r.ingest_documents(
            [
                Document(
                    id=generate_id_from_label(company),
                    type="txt",
                    data=company_data,
                    metadata={},
                )
            ]
        )
    except:
        continue
    i += 1

print_all_relationships(kg_provider)
```

Let's start by scraping one example to get an initial sense of the data quality produced by this technique.

```bash
python r2r/examples/scripts/build_yc_kg.py --max_entries=1
```

Running the command above will produce output like the following:

```bash
Joe Gebbia -[FOUNDED]-> Airbnb
Airbnb -[LOCATION]-> San Francisco
Airbnb -[HAS]-> 7M listings
Airbnb -[LOCATION]-> 191+ countries
Joe Gebbia -[WORKED_AT]-> Airbnb
Joe Gebbia -[FOUNDED]-> Samara
Samara -[PRODUCT]-> fully customized, factory-made homes
Airbnb -[FOUNDED]-> August
Airbnb -[FOUNDED]-> 2008
Airbnb -[LOCATION]-> California
Airbnb -[HAS]-> 33,000 cities
Airbnb -[HAS]-> 192 countries
Airbnb -[LOCATION]-> Dublin
Airbnb -[LOCATION]-> London
Airbnb -[LOCATION]-> Barcelona
Airbnb -[LOCATION]-> Paris
Airbnb -[LOCATION]-> Milan
Airbnb -[LOCATION]-> Copenhagen
Airbnb -[LOCATION]-> Berlin
Airbnb -[LOCATION]-> Moscow
Airbnb -[LOCATION]-> São Paolo
Airbnb -[LOCATION]-> Sydney
Airbnb -[LOCATION]-> Singapore
Airbnb -[EDUCATED_AT]-> Y Combinator
Airbnb -[PARTICIPATED]-> W09
Airbnb -[INDUSTRY]-> marketplace
Airbnb -[INDUSTRY]-> travel
Airbnb -[MEDIA]-> http://airbnb.com
Brian Chesky -[WORKED_AT]-> Airbnb
Brian Chesky -[POSITION]-> CEO
...
```

We are now ready to run a much larger ingestion.

```bash
poetry run python r2r/examples/scripts/build_yc_kg.py --delete --max_entries=100
```

### Graph Visualization


After successful completion we will see a large printout showing the various connections that have been formed across our knowledge graph. Indeed, there are more connections than we can readily understand. We use the Neo4j browser which ships with the desktop application to gain us some intuition around the constructed graph.

<br />

<figure>
  <img src="/graph.png" alt="Industry Relationships"/>
  <figcaption>Figure 1: Graph showing industry relationships.</figcaption>
</figure>

<br />

We can see a rich and nuanced structure has naturally emerged, with SaaS being the centralmost node.
<figure>
  <img src="/graph2.png" alt="Founder Relationships"/>
  <figcaption>Figure 2: Graph showing founder relationships, focused on Owen Colegrove.</figcaption>
</figure>


Using the snippets that follow we can begin to explore the graph through targeted queries.

```python

def execute_query(provider, query, params={}):
    print(f"Executing query: {query}")
    with provider.client.session(database=provider._database) as session:
        result = session.run(query, params)
        return [record.data() for record in result]

```


Executing a few queries, we see outputs like the following:

```bash
## Locating all founders

Executing query: 
MATCH (p:PERSON)-[:FOUNDED]->(c)
RETURN p.id AS Founder, c.id AS Company
ORDER BY c.id
LIMIT 10;

Results:
[{'Founder': 'Nathan Blecharczyk', 'Company': 'Airbnb'}, {'Founder': 'Brian Chesky', 'Company': 'Airbnb'}, {'Founder': 'Joe Gebbia', 'Company': 'Airbnb'}, {'Founder': 'Tommy Guo', 'Company': 'Airfront'}, {'Founder': 'Joanne Wang', 'Company': 'Airfront'}, {'Founder': 'Adam Tilton', 'Company': 'Aktive'}, {'Founder': 'Abraham Heifets', 'Company': 'Atomwise'}, {'Founder': 'Nicholas Charriere', 'Company': 'Axilla'}, {'Founder': 'Caitlin', 'Company': 'B2B marketing software'}, {'Founder': 'Timmy', 'Company': 'B2B marketing software'}]
```


```bash
## Locating 2-time founders

Executing query: 
MATCH (p:PERSON)-[:FOUNDED]->(c:ORGANIZATION)
WITH p.id AS Person, COUNT(c) AS CompaniesCount
RETURN Person, CompaniesCount
ORDER BY CompaniesCount DESC
LIMIT 10;

Results:
[{'Person': 'Ilana Nasser', 'CompaniesCount': 3}, {'Person': 'Eric', 'CompaniesCount': 2}, {'Person': 'Kris Pahuja', 'CompaniesCount': 2}, {'Person': 'Sam', 'CompaniesCount': 2}, {'Person': 'Tom Blomfield', 'CompaniesCount': 2}, {'Person': 'Umur Cubukcu', 'CompaniesCount': 2}, {'Person': 'Jason', 'CompaniesCount': 2}, {'Person': 'Joe Gebbia', 'CompaniesCount': 2}, {'Person': 'Adam Tilton', 'CompaniesCount': 2}, {'Person': 'Alex', 'CompaniesCount': 2}]
```

```bash
### Locating companies with AI products

Executing query: 
MATCH (c:ORGANIZATION)-[r:PRODUCT]->(t)
WHERE t.id CONTAINS 'AI'
RETURN DISTINCT c.id AS Company, t.id AS Product
ORDER BY c.id
LIMIT 10;

Results:
[{'Company': 'AgentsForce', 'Product': 'AI support agents'}, {'Company': 'Airfront', 'Product': 'AI-first email platform'}, {'Company': 'Airfront', 'Product': 'AI-first email platform with built-in automations'}, {'Company': 'Airfront', 'Product': 'AI automation platform'}, {'Company': 'Axflow', 'Product': 'AI app'}, {'Company': 'Clarum', 'Product': 'AI-powered due diligence solutions'}, {'Company': 'Clarum', 'Product': 'AI-powered due diligence'}, {'Company': 'CommodityAI', 'Product': 'AI-automation platform'}, {'Company': 'Dawn', 'Product': 'Analytics for AI products'}, {'Company': 'Decipher', 'Product': 'AI-powered user impact summaries'}]```
```

```bash
## Locating AI founders with optional info

Executing query: 
MATCH (p:PERSON)-[:FOUNDED]->(c:ORGANIZATION)-[:INDUSTRY]->(i:INDUSTRY)
WHERE i.id CONTAINS 'saas'
OPTIONAL MATCH (p)-[:EDUCATED_AT]->(s:ORGANIZATION)
OPTIONAL MATCH (p)-[:WORKED_AT]->(r:ORGANIZATION)
OPTIONAL MATCH (c)-[:PRODUCT]->(t:PRODUCT)
RETURN p.id AS Founder, c.id AS Company, collect(DISTINCT i.id) AS Industries, collect(DISTINCT t.id) AS Products, collect(DISTINCT s.id) AS Schools, collect(DISTINCT r.id) AS Companies
ORDER BY p.id
LIMIT 10;

Results:
[{'Founder': 'Abdullah Al Zandani', 'Company': 'Coperniq', 'Industries': ['saas'], 'Products': ['workflow software', 'Workflow software'], 'Schools': [], 'Companies': ['two solar companies', 'ADM']}, {'Founder': 'Akshit Khurana', 'Company': 'Narrative', 'Industries': ['saas'], 'Products': [], 'Schools': [], 'Companies': ['Ginger.io', 'Housecanary', 'Pytorch', 'Meta', 'SlideShare', 'Ambient AI', 'LinkedIn']}, {'Founder': 'Alan', 'Company': 'Health Harbor', 'Industries': ['saas'], 'Products': ['Automated billing for private medical practices', 'billing'], 'Schools': ['Yale'], 'Companies': ['Nuro', 'Meta']}, {'Founder': 'Daiyi Yang', 'Company': 'Empower', 'Industries': ['saas'], 'Products': ['Empower-Functions', 'LLMs'], 'Schools': [], 'Companies': ['Meta', 'Revinate']}, {'Founder': 'Dani Penev', 'Company': 'Slicker', 'Industries': ['saas'], 'Products': ['AI-powered payments platform', 'payments integration'], 'Schools': [], 'Companies': ['Thought Machine', 'Stripe']}, {'Founder': 'David Hua', 'Company': 'Meadow', 'Industries': ['saas'], 'Products': [], 'Schools': [], 'Companies': []}, {'Founder': 'Hansen Qian', 'Company': 'Lightski', 'Industries': ['saas'], 'Products': ['User-facing analytics', 'Lightski AI Data Scientist', 'embedded data analytics', 'analytics experience', 'AI Data Scientist', 'Lightski'], 'Schools': [], 'Companies': ['Affinity CRM']}, {'Founder': 'Ivan Valkov', 'Company': 'Slicker', 'Industries': ['saas'], 'Products': ['AI-powered payments platform', 'payments integration'], 'Schools': [], 'Companies': []}, {'Founder': 'Jacob', 'Company': 'IcePanel', 'Industries': ['saas'], 'Products': ['collaborative modelling tool', 'relationships', 'Dynamic flows', 'diagrams', 'IcePanel objects', 'C4 model', 'single source of truth', 'Levelled diagramming', 'software architecture', 'reusable objects', 'SaaS tool'], 'Schools': [], 'Companies': []}, {'Founder': 'Jean-Michael Diei', 'Company': 'PropRise', 'Industries': ['saas'], 'Products': ['Beacon', 'Beacon by PropRise'], 'Schools': [], 'Companies': []}]
```

