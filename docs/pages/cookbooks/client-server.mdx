import { Callout } from 'nextra/components'

## R2R Client-Server Guide

### Setting Up the Server

**This document extends the [R2R Quickstart](/getting-started/quickstart)** by demonstrating how to run R2R with a Client-Server architecture. The R2R server can be stood up to handle requests, while the client can communicate with the server to perform various operations. The R2R server API can be [viewed here](/getting-started/r2r-api).

<Callout type="warning" emoji="🐳">
If you are running with Docker, then you may skip the step below
</Callout>

**Starting the a server**:
   Ensure you have all necessary dependencies installed as described in the [installation guide](/getting-started/installation). Then, use the following command to start the server:
   ```bash filename="bash" copy
   python -m r2r.examples.quickstart serve
   ```
   This command starts the R2R server on the default host `0.0.0.0` and port `8000`, see the output below.


   ```bash filename="User Terminal"

r2r.core.providers.vector_db_provider - INFO - Initializing VectorDBProvider with config extra_fields={} provider='pgvector' collection_name='123_demo_vecs_123'. - 2024-06-19 16:02:34,612
r2r.core.providers.embedding_provider - INFO - Initializing EmbeddingProvider with config extra_fields={'text_splitter': {'type': 'recursive_character', 'chunk_size': 512, 'chunk_overlap': 20}} provider='openai' base_model='text-embedding-3-small' base_dimension=512 rerank_model=None rerank_dimension=None rerank_transformer_type=None batch_size=128. - 2024-06-19 16:02:35,512
r2r.core.providers.llm_provider - INFO - Initializing LLM provider with config: extra_fields={} provider='litellm' - 2024-06-19 16:02:36,009
   INFO:     Started server process [55001]
   INFO:     Waiting for application startup.
   INFO:     Application startup complete.
   INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
   ```
### Using the Client

The R2R engine includes a [python client](https://github.com/SciPhi-AI/R2R/blob/main/r2r/main/r2r_client.py) and a [typescript client](https://github.com/SciPhi-AI/r2r-js/blob/main/src/r2rClient.ts). All of the quickstart commands can be run with the python client using the `--client_server_mode` parameter to run the demo in Client-Server mode. Note that these commands will only run successfully if the server is active at http://0.0.0.0:8000.

<Callout type="warning" emoji="️⚠️">
   Be sure to run client commands in a new terminal!
</Callout>

### Example Commands

1. **Ingest Documents as Files**:
   ```bash filename="bash" copy
   python -m r2r.examples.quickstart ingest_as_files --client_server_mode
   ```
   This command will send the ingestion request to the server running at `http://localhost:8000`.

2. **Perform a Search**:
   ```bash filename="bash" copy
   python -m r2r.examples.quickstart search --query="Who was Aristotle?" --client_server_mode
   ```
   This command sends the search query to the server and retrieves the results.

3. **Run a RAG Completion**:
   ```bash filename="bash" copy
   python -m r2r.examples.quickstart rag --query="What was Uber's profit in 2020?" --client_server_mode
   ```
   This command sends the RAG query to the server and retrieves the generated response.

4. **Run a RAG Stream**:
   ```bash filename="bash" copy
   python -m r2r.examples.quickstart rag --query="What was Lyft's profit in 2020?" --streaming=true --client_server_mode
   ```
   This command streams the RAG query results from the server.

## Summary

By using the Client-Server model, you can extend the basic R2R quickstart to support more scalable and modular deployments. The server handles requests and performs heavy computations, while clients can communicate with the server to perform ingestion, search, RAG, and other operations, as shown in the examples above. For detailed setup and basic functionality, refer back to the [R2R Quickstart](/getting-started/quickstart).
