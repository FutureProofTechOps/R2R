# Evaluation Pipeline

The Evaluation Pipeline is responsible for evaluating the quality of generated completions by comparing them against the original query and context. It supports different evaluation providers and allows for sampling-based evaluation.

## Basic Evaluation Pipeline

The `BasicEvalPipeline` is an implementation of the `EvalPipeline` abstract base class. It provides a straightforward way to evaluate completions using a specified evaluation provider.

### Initialization

The `BasicEvalPipeline` is initialized with the following parameters:

- `eval_config`: A dictionary containing the configuration for the evaluation pipeline.
  - `frequency`: The frequency at which evaluations should be performed (e.g., 0.25 for 25% of the time).
  - `provider`: The name of the evaluation provider to use (e.g., "deepeval" or "parea").
- `logging_connection` (optional): An instance of `LoggingDatabaseConnection` for logging.

### Supported Evaluation Providers

The `BasicEvalPipeline` supports the following evaluation providers:

- `deepeval`: Uses the DeepEval library for evaluation.
- `parea`: Uses the PAREA (Prompt And Response Evaluation API) for evaluation.

### Pipeline Execution

The `run` method executes the evaluation pipeline. It takes the following parameters:

- `query`: The original query used for generating the completion.
- `context`: The context used for generating the completion.
- `completion`: The generated completion to be evaluated.
- `run_id` (optional): A unique identifier for the evaluation run.

The pipeline performs the following steps:

1. Initializes the pipeline by calling the `initialize_pipeline` method.
2. Checks if the specified evaluation provider is supported.
3. Imports the necessary evaluation provider module based on the configuration.
4. Calls the `evaluate` method of the evaluation provider to evaluate the completion.
5. Returns the evaluation result if the evaluation is performed based on the specified frequency.

### Evaluation

The `evaluate` method is decorated with `@log_execution_to_db` to log the evaluation execution to the database. It takes the `query`, `context`, and `completion` as parameters and returns the evaluation result.

## Evaluation Provider

The `EvalProvider` class provides a generic interface for evaluation providers. It supports sampling-based evaluation, where the evaluation is performed based on a specified sampling fraction.

### Initialization

The `EvalProvider` is initialized with the following parameters:

- `provider`: The name of the evaluation provider.
- `sampling_fraction` (optional): The fraction of evaluations to perform (default is 1.0, meaning all evaluations are performed).

### Evaluation

The `evaluate` method of the `EvalProvider` class takes the `query`, `context`, and `completion` as parameters and returns the evaluation result if the evaluation is performed based on the specified sampling fraction.

## Custom Evaluation Pipeline

To create a custom evaluation pipeline, you can subclass the `EvalPipeline` abstract base class and implement the `evaluate` method. Here's an example of a custom evaluation pipeline that uses a different evaluation provider:

```python
from typing import Any
from r2r.core import EvalPipeline, log_execution_to_db
from my_custom_eval_provider import MyCustomEvalProvider

class CustomEvalPipeline(EvalPipeline):
    def __init__(self, eval_config: dict, *args, **kwargs):
        frequency = eval_config["frequency"]
        super().__init__(frequency, *args, **kwargs)
        self.eval_provider = MyCustomEvalProvider()

    @log_execution_to_db
    def evaluate(self, query: str, context: str, completion: str) -> Any:
        return self.eval_provider.evaluate(query, context, completion)
```

In this example, the `CustomEvalPipeline` uses a custom evaluation provider called `MyCustomEvalProvider`. The `evaluate` method is implemented to call the `evaluate` method of the custom evaluation provider.

## Passing the Custom Evaluation Pipeline to the Factory

To use the custom evaluation pipeline with the `E2EPipelineFactory`, you can pass it when creating the pipeline:

```python
from r2r.main import E2EPipelineFactory
from my_custom_pipeline import CustomEvalPipeline

app = E2EPipelineFactory.create_pipeline(
    config=config,
    eval_pipeline_impl=CustomEvalPipeline,
)
```

By passing the `CustomEvalPipeline` to the `create_pipeline` method using the `eval_pipeline_impl` parameter, the factory will use the custom evaluation pipeline instead of the default `BasicEvalPipeline`.

That's it! You now have a basic understanding of the Evaluation Pipeline, how to create a custom evaluation pipeline, and how to pass it to the `E2EPipelineFactory` for use in the end-to-end pipeline.